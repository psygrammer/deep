{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Graph Structures,  Printing/Drawing Theano graphs, Derivatives in Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graph Structures\n",
    "* Printing/Drawing Theano graphs\n",
    "* Derivatives in Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Theano Graphs\n",
    "* Automatic Differentiation\n",
    "* Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This chapter is meant to introduce you to a required minimum of the inner workings of Theano. \n",
    "For more detail see Extending Theano.\n",
    "    - variables : The first step in writing Theano code is to write down all mathematical relations using symbolic placeholders (variables). \n",
    "    - ops : When writing down these expressions you use operations like +, -, **, sum(), tanh(). All these are represented internally as ops.\n",
    "* Theano builds internally a graph structure composed of interconnected\n",
    "    - variable nodes\n",
    "    - op nodes\n",
    "    - apply nodes - An apply node represents the application of an op to some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://deeplearning.net/software/theano/_images/apply1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction between instances of Apply (blue), Variable (red), Op (green), and Type (purple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "x = theano.tensor.dmatrix('x')\n",
    "y = x * 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elemwise{mul,no_inplace}(x, DimShuffle{x,x}.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.gof.graph.Apply"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y.owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elemwise{mul,no_inplace}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.op.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y.owner.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DimShuffle{x,x}.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.tensor.var.TensorVariable"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y.owner.inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.gof.graph.Apply"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y.owner.inputs[1].owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<theano.tensor.elemwise.DimShuffle at 0x104f8ed50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[1].owner.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorConstant{2.0}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[1].owner.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Having the graph structure, computing automatic differentiation is simple. The only thing tensor.grad() has to do is to traverse the graph from the outputs back towards the inputs through all apply nodes (apply nodes are those that define which computations the graph does). \n",
    "* For each such apply node, its op defines how to compute the gradient of the node’s outputs with respect to its inputs. Note that if an op does not provide this information, it is assumed that the gradient is not defined. \n",
    "* Using the chain rule these gradients can be composed in order to obtain the expression of the gradient of the graph’s output with respect to the graph’s inputs ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = theano.tensor.vector(\"a\")      # declare symbolic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a + a ** 10                    # build symbolic expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = theano.function([a], b)        # compile function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.     2.  1026.]\n"
     ]
    }
   ],
   "source": [
    "print f([0, 1, 2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unoptimized graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 팁 ! pydot 관련 에러가 나면 \n",
    "    - http://stackoverflow.com/questions/15951748/pydot-and-graphviz-error-couldnt-import-dot-parser-loading-of-dot-files-will\n",
    "    - 즉 1) pydot의 pyparsing 버전을 맞춰준다. 2) graphviz를 시스템에 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at ./symbolic_graph_unopt.png\n"
     ]
    }
   ],
   "source": [
    "theano.printing.pydotprint(b, outfile=\"./symbolic_graph_unopt.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./symbolic_graph_unopt.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimized graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at ./symbolic_graph_opt.png\n"
     ]
    }
   ],
   "source": [
    "theano.printing.pydotprint(f, outfile=\"./symbolic_graph_opt.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./symbolic_graph_opt.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing/Drawing Theano graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pretty Printing\n",
    "* Debug Printing\n",
    "* Picture Printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Theano provides the functions \n",
    "    - theano.printing.pprint() \n",
    "    - and theano.printing.debugprint() to print a graph to the terminal before or after compilation. \n",
    "* pprint() is more compact and math-like, debugprint() is more verbose. \n",
    "* Theano also provides pydotprint() that creates an image of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consider again the logistic regression example:\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "N = 400\n",
    "feats = 784\n",
    "D = (rng.randn(N, feats).astype(theano.config.floatX), rng.randint(size=N,low=0, high=2).astype(theano.config.floatX))\n",
    "training_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "w = theano.shared(rng.randn(feats).astype(theano.config.floatX), name=\"w\")\n",
    "b = theano.shared(numpy.asarray(0., dtype=theano.config.floatX), name=\"b\")\n",
    "x.tag.test_value = D[0]\n",
    "y.tag.test_value = D[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct Theano expression graph\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w)-b)) # Probability of having a one\n",
    "prediction = p_1 > 0.5 # The prediction that is done: 0 or 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1) # Cross-entropy\n",
    "cost = xent.mean() + 0.01*(w**2).sum() # The cost to optimize\n",
    "gw,gb = T.grad(cost, [w,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and prediction function\n",
    "train = theano.function(inputs=[x,y], outputs=[prediction, xent], updates=[[w, w-0.01*gw], [b, b-0.01*gb]], name = \"train\")\n",
    "predict = theano.function(inputs=[x], outputs=prediction, name = \"predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gt((TensorConstant{1} / (TensorConstant{1} + exp(((-(x \\\\dot w)) - b)))), TensorConstant{0.5})'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.printing.pprint(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{gt,no_inplace} [@A] ''   \n",
      " |Elemwise{true_div,no_inplace} [@B] ''   \n",
      " | |DimShuffle{x} [@C] ''   \n",
      " | | |TensorConstant{1} [@D]\n",
      " | |Elemwise{add,no_inplace} [@E] ''   \n",
      " |   |DimShuffle{x} [@F] ''   \n",
      " |   | |TensorConstant{1} [@D]\n",
      " |   |Elemwise{exp,no_inplace} [@G] ''   \n",
      " |     |Elemwise{sub,no_inplace} [@H] ''   \n",
      " |       |Elemwise{neg,no_inplace} [@I] ''   \n",
      " |       | |dot [@J] ''   \n",
      " |       |   |x [@K]\n",
      " |       |   |w [@L]\n",
      " |       |DimShuffle{x} [@M] ''   \n",
      " |         |b [@N]\n",
      " |DimShuffle{x} [@O] ''   \n",
      "   |TensorConstant{0.5} [@P]\n"
     ]
    }
   ],
   "source": [
    "# The pre-compilation graph:\n",
    "theano.printing.debugprint(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{Composite{GT(scalar_sigmoid((-((-i0) - i1))), i2)}} [@A] ''   4\n",
      " |CGemv{inplace} [@B] ''   3\n",
      " | |Alloc [@C] ''   2\n",
      " | | |TensorConstant{0.0} [@D]\n",
      " | | |Shape_i{0} [@E] ''   1\n",
      " | |   |x [@F]\n",
      " | |TensorConstant{1.0} [@G]\n",
      " | |x [@F]\n",
      " | |w [@H]\n",
      " | |TensorConstant{0.0} [@D]\n",
      " |InplaceDimShuffle{x} [@I] ''   0\n",
      " | |b [@J]\n",
      " |TensorConstant{(1,) of 0.5} [@K]\n"
     ]
    }
   ],
   "source": [
    "# The post-compilation graph:\n",
    "theano.printing.debugprint(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picture Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at logreg_pydotprint_prediction.png\n"
     ]
    }
   ],
   "source": [
    "# The pre-compilation graph:\n",
    "theano.printing.pydotprint(prediction, outfile=\"logreg_pydotprint_prediction.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg_pydotprint_prediction.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at logreg_pydotprint_predict.png\n"
     ]
    }
   ],
   "source": [
    "# The post-compilation graph:\n",
    "theano.printing.pydotprint(predict, outfile=\"logreg_pydotprint_predict.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg_pydotprint_predict.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at logreg_pydotprint_train.png\n"
     ]
    }
   ],
   "source": [
    "# The optimized training graph:\n",
    "theano.printing.pydotprint(train, outfile=\"logreg_pydotprint_train.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg_pydotprint_train.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Derivatives in Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computing Gradients\n",
    "* Computing the Jacobian\n",
    "* Computing the Hessian\n",
    "* Jacobian times a Vector\n",
    "    - R-operator\n",
    "    - L-operator\n",
    "* Hessian times a Vector\n",
    "* Final Pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use Theano for a slightly more sophisticated task: create a function which computes the derivative of some expression y with respect to its parameter x. To do this we will use the macro T.grad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://deeplearning.net/software/theano/_images/math/7e4e2058a694a404e828a8d7ab97827b2ed9cd25.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import pp\n",
    "x = T.dscalar('x')\n",
    "y = x ** 2\n",
    "gy = T.grad(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(gy)  # print out the gradient prior to optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = theano.function([x], gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the gradient of complex expressions such as the logistic function defined above. It turns out that the derivative of the logistic is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://deeplearning.net/software/theano/_images/math/82020178fdc03e96814d00b3e2391c6f7270ce5c.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://deeplearning.net/software/theano/_images/dlogistic.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dmatrix('x')\n",
    "s = T.sum(1 / (1 + T.exp(-x)))\n",
    "gs = T.grad(s, x)\n",
    "dlogistic = theano.function([x], gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25      ,  0.19661193],\n",
       "       [ 0.19661193,  0.10499359]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogistic([[0, 1], [-1, -2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano’s parlance, the term Jacobian designates the tensor comprising the first partial derivatives of the output of a function with respect to its inputs. (This is a generalization of to the so-called Jacobian matrix in Mathematics.) Theano implements the theano.gradient.jacobian() macro that does all that is needed to compute the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moodern/.venv/deepy/lib/python2.7/site-packages/theano/gof/cmodule.py:293: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  rval = __import__(module_name, {}, {}, [module_name])\n"
     ]
    }
   ],
   "source": [
    "x = T.dvector('x')\n",
    "y = x ** 2\n",
    "J, updates = theano.scan(lambda i, y,x : T.grad(y[i], x), sequences=T.arange(y.shape[0]), non_sequences=[y,x])\n",
    "f = theano.function([x], J, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.,  0.],\n",
       "       [ 0.,  8.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano, the term Hessian has the usual mathematical acception: It is the matrix comprising the second order partial derivative of a function with scalar output and vector input. Theano implements theano.gradient.hessian() macro that does all that is needed to compute the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.dvector('x')\n",
    "y = x ** 2\n",
    "cost = y.sum()\n",
    "gy = T.grad(cost, x)\n",
    "H, updates = theano.scan(lambda i, gy,x : T.grad(gy[i], x), sequences=T.arange(gy.shape[0]), non_sequences=[gy, x])\n",
    "f = theano.function([x], H, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.],\n",
       "       [ 0.,  2.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian times a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R operator is built to evaluate the product between a Jacobian and a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = T.dmatrix('W')\n",
    "V = T.dmatrix('V')\n",
    "x = T.dvector('x')\n",
    "y = T.dot(x, W)\n",
    "JV = T.Rop(y, W, V)\n",
    "f = theano.function([W, V, x], JV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([[1, 1], [1, 1]], [[2, 2], [2, 2]], [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In similitude to the R-operator, the L-operator would compute a row vector times the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = T.dmatrix('W')\n",
    "v = T.dvector('v')\n",
    "x = T.dvector('x')\n",
    "y = T.dot(x, W)\n",
    "VJ = T.Lop(y, W, v)\n",
    "f = theano.function([v,x], VJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 2.,  2.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([2, 2], [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian times a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to compute the Hessian times a vector, you can make use of the above-defined operators to do it more efficiently than actually computing the exact Hessian and then performing the product. Due to the symmetry of the Hessian matrix, you have two options that will give you the same result, though these options might exhibit differing performances. Hence, we suggest profiling the methods before using either one of the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dvector('x')\n",
    "v = T.dvector('v')\n",
    "y = T.sum(x ** 2)\n",
    "gy = T.grad(y, x)\n",
    "vH = T.grad(T.sum(gy * v), x)\n",
    "f = theano.function([x, v], vH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  4.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4], [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# or, making use of the R-operator:\n",
    "x = T.dvector('x')\n",
    "v = T.dvector('v')\n",
    "y = T.sum(x ** 2)\n",
    "gy = T.grad(y, x)\n",
    "Hv = T.Rop(gy, x, v)\n",
    "f = theano.function([x, v], Hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  4.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([4, 4], [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The grad function works symbolically: it receives and returns Theano variables.\n",
    "* grad can be compared to a macro since it can be applied repeatedly.\n",
    "* Scalar costs only can be directly handled by grad. Arrays are handled through repeated applications.\n",
    "* Built-in functions allow to compute efficiently vector times Jacobian and vector times Hessian.\n",
    "* Work is in progress on the optimizations required to compute efficiently the full Jacobian and the Hessian matrix as well as the Jacobian times vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Theano 0.7 Tutorial - http://deeplearning.net/software/theano/tutorial/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
